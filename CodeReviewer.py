import json
import os
import logging
from typing import Dict, List, Any, Optional, Tuple
import google.generativeai as genai #type: ignore

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('DataSynthesizer')

class CodeReviewer:
    """
    Component responsible for performing code review and static analysis
    of the generated data synthesis code using LLM (Gemini 2.5 Pro Preview).
    """
    
    def __init__(self, api_key: str):
        """
        Initialize the Code Reviewer component.
        """
        self.api_key = api_key
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(
            #model_name="models/gemini-2.5-pro-preview-05-06"
            model_name="models/gemini-2.5-flash-preview-04-17"
        )
        
        logger.info(f"Code Reviewer initialized with {self.model.model_name}")
    
    def _load_generated_code(self, code_path: str) -> str:
        """
        Load the generated code file.
        """
        logger.info(f"Loading generated code from {code_path}")
        try:
            with open(code_path, 'r', encoding='utf-8') as f:
                code = f.read()
            logger.info(f"Successfully loaded generated code ({len(code)} characters)")
            return code
        except Exception as e:
            logger.error(f"Error loading generated code: {str(e)}")
            raise
    
    def _load_requirements(self, requirements_path: str) -> str:
        """
        Load the requirements document generated by the LLM Analyzer.
        """
        logger.info(f"Loading requirements from {requirements_path}")
        try:
            with open(requirements_path, 'r', encoding='utf-8') as f:
                requirements = f.read()
            logger.info("Successfully loaded requirements document")
            return requirements
        except Exception as e:
            logger.error(f"Error loading requirements: {str(e)}")
            raise
    
    def _construct_code_review_prompt(self, code: str, requirements: str) -> str:
        """
        Construct a detailed prompt for the LLM to perform code review.
        """
        logger.info("Constructing code review prompt")
        
        # Construct the full prompt
        prompt = f"""
You are an expert Python code reviewer specializing in data generation and synthesis code. 
Your task is to perform a thorough static analysis of the provided Python code to identify
issues, errors, and ensure it meets the specified requirements.

# Code to Review
```python
{code}
```

# Requirements to Validate Against
```
{requirements}
```

# Review Instructions

Please perform a comprehensive code review focusing on the following aspects (the coder was only given the same Requirements as you do, so if the code improvise due to lack of information, don't be too hard):

1. **Syntax and Basic Structure**:
   - Check for syntax errors
   - Validate import statements
   - Examine function and class definitions
   - Look for indentation or formatting issues

2. **Functional Completeness**:
   - Does the code implement all columns specified in the requirements?
   - Does the code follow the requirements for each column?
   - Are relationships between columns properly handled?
   - Does the code adhere to the column generation order in the requirements?

3. **Implementation of Requirements**:
   - For each column in the requirements, verify that:
     - The specified generation strategy is used
     - Required libraries and providers are properly imported and used
     - Patterns and constraints are respected
     - Dependencies between columns are handled correctly

4. **Data Quality Checks**:
   - Are there mechanisms to validate generated data?
   - Are edge cases handled properly?
   - Is there adequate error handling?

5. **Execution Readiness**:
   - Are there potential runtime errors or exceptions?
   - Is the code runnable without modifications?
   - Are there any missing dependencies or assumptions?

6. **Efficiency and Best Practices**:
   - Are there any performance concerns?
   - Does the code follow Python best practices?
   - Is there adequate commenting and documentation?

# Output Format

Please provide your analysis in the following JSON format:

```json
{{
  "pass": true/false,
  "summary": "Short summary of the review",
  "critical_issues": [
    {{
      "section_tag": "imports_setup/all_column_logic/main_orchestration/file_output",
      "type": "syntax/functional/requirements/etc.",
      "description": "Detailed description of the issue",
      "traceback": "If this issue directly relates to an error message or traceback seen during execution (check 'Code Execution Context' if provided), include the specific error message.",
      "location": "Line numbers or function name",
      "severity": "critical",
      "recommendation": "How to fix the issue. Be VERY specific and actionable. If possible, suggest the exact line change (e.g., 'Change line X from Y to Z', or 'Add library X to imports'). Avoid vague recommendations."
    }}
  ],
  "non_critical_issues": [
    {{
      "section_tag": "imports_setup/all_column_logic/main_orchestration/file_output",
      "type": "performance/style/etc.",
      "description": "Detailed description of the issue",
      "location": "Line numbers or function name",
      "severity": "warning/suggestion",
      "recommendation": "How to improve this aspect"
    }}
  ],
  "section_statuses": [
    {{
      "section_tag": "imports_setup/all_column_logic/main_orchestration/file_output",
      "status": "okay/needs_revision/partially_okay",
      "reason": "Brief justification if not 'okay', e.g., 'Minor import issue found', or 'Column X logic is faulty'",
      "relevant_issue_descriptions": ["summary of issue 1", "summary of issue 2"]
    }}
  ],
  "requirement_validations": [
    {{
      "requirement": "Column X with pattern Y",
      "implemented": true/false,
      "notes": "Notes on implementation correctness"
    }}
  ],
  "overall_assessment": "Detailed overall assessment of code quality and compliance",
  "passed_columns": ["COLUMN_A_NAME_IF_IT_PASSED", "COLUMN_B_NAME_IF_IT_PASSED"] // List of column names whose generation logic passed validation and should not be modified. Provide an empty list if none definitively passed. This is derived from the 'all_column_logic' section's success.
}}
```
**Important for `section_tag`**: Use one of these exact tags:
- `imports_setup`: For issues related to imports, logging setup, constants.
- `all_column_logic`: For issues within any of the column generation functions or their interdependencies.
- `main_orchestration`: For issues in the main data generation loop, DataFrame assembly, or overall flow.
- `file_output`: For issues related to saving the final data to a file.
- If the issue spans multiple sections, make separate entries in the JSON output, one for each section.

For 'section_statuses', evaluate each of the four main sections. If `all_column_logic` has some columns passing and some failing, it might be 'partially_okay' or 'needs_revision' depending on severity.

Ensure your JSON output is properly formatted and valid. The code "passes" static analysis if it has no critical issues that would prevent successful execution or generation of data meeting the requirements.
"""
        return prompt
    
    def review_code(self, code_path: str, requirements_path: str, execution_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
    Perform static analysis and code review of the generated code,
    with optional execution/validation context.
        """
        logger.info("Starting code review process")

        code = self._load_generated_code(code_path)
        requirements = self._load_requirements(requirements_path)

        # Add execution context to the prompt
        exec_context_str = ""
        if execution_context:
            exec_context_str += "This is the result of the code execution, review the code with this context in mind and avoid giving contradicting feedback.\n"
            exec_context_str += "\n\n# Code Execution Context\n"
            exec_context_str += f"Execution success: {execution_context.get('overall_success', False)}\n"
            exec_context_str += f"Execution summary: {execution_context.get('summary', '')}\n"

            if execution_context.get("execution", {}).get("traceback"): # Access traceback correctly
                exec_context_str += f"\nTraceback:\n{execution_context['execution']['traceback']}\n"
                exec_context_str += "If a traceback is provided above, analyze it carefully to identify the root cause of the execution failure and ensure your review recommendations directly address it.\n"

            validation_data = execution_context.get("validation", {})
            if isinstance(validation_data, dict):
                validations = validation_data.get("validations", [])
                if validations:
                    exec_context_str += "\nValidation Failures:\n"
                    for v in validations:
                        if not v.get("passed", True):
                            exec_context_str += f"- {v.get('name', '')} on {v.get('column', '')}: {v.get('details', '')}\n"

        # Add review guidance to encourage precise and non-redundant critique
        review_guidance = """
# Review Guidelines:
- Focus only on issues that would lead to execution failure or data validation failure.
- Avoid nitpicking or recommending stylistic changes unless necessary.
- If parts of the code are working and validated, acknowledge them.
- Populate the "passed_columns" field in the JSON output with a list of column names that passed validation. These are columns whose core data generation logic (creation-wise, not order-wise, within the 'all_column_logic' section) met requirements and did not cause errors. If no columns definitively passed, provide an empty list [] in the "passed_columns" field.
"""

        prompt = exec_context_str + self._construct_code_review_prompt(code, requirements) + review_guidance

        try:
            logger.info("Sending review prompt to Gemini with execution context")
            response = self.model.generate_content(prompt)
            review_results = self._parse_code_review_response(response.text)
            logger.info("Successfully completed contextual code review")
            return review_results

        except Exception as e:
            logger.error(f"Error during review: {str(e)}")
            return {
                "pass": False,
                "summary": f"Error in review: {str(e)}",
                "critical_issues": [{
                    "type": "review_error",
                    "description": f"Review exception: {str(e)}",
                    "severity": "critical",
                    "recommendation": "Check model availability and inputs."
                }],
                "non_critical_issues": [],
                "section_statuses": [],
                "overall_assessment": "Review failed.",
                "passed_columns": []
            }

    
    def _parse_code_review_response(self, response_text: str) -> Dict[str, Any]:
        """
        Parse the LLM response to extract structured code review results.
        """
        try:
            # Try to find and extract JSON content from the response
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response_text[json_start:json_end]
                review_results = json.loads(json_str)
                if 'section_statuses' not in review_results:
                    review_results['section_statuses'] = []
                return review_results
            else:
                # If no JSON found, log an error and return error structure
                logger.error("No valid JSON found in LLM response")
                return {
                    "pass": False,
                    "summary": "Failed to parse LLM response",
                    "critical_issues": [{
                        "type": "parsing_error",
                        "description": "Could not parse LLM response as JSON",
                        "severity": "critical",
                        "recommendation": "Check LLM prompt and response format"
                    }],
                    "non_critical_issues": [],
                    "section_statuses": [],
                    "overall_assessment": "Code review failed due to response parsing error",
                    "passed_columns": [] 
                }
                
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing LLM response as JSON: {str(e)}")
            logger.debug(f"Response text: {response_text}")
            # Return error structure on JSON parse error
            return {
                "pass": False,
                "summary": "Failed to parse LLM response as JSON",
                "critical_issues": [{
                    "type": "json_decode_error",
                    "description": f"JSON decode error: {str(e)}",
                    "severity": "critical",
                    "recommendation": "Check LLM response format"
                }],
                "non_critical_issues": [],
                "section_statuses": [],
                "overall_assessment": "Code review failed due to JSON parsing error",
                "passed_columns": []
            }
    
    def save_review_results(self, review_results: Dict[str, Any], output_path: str = "pipeline_run_outputs/code_review.json") -> str:
        """
        Save the code review results to a file.
            
        Returns:
            Path to the saved file
        """
        logger.info(f"Saving code review results to {output_path}")
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(review_results, f, ensure_ascii=False, indent=2)
            logger.info(f"Code review results successfully saved to {output_path}")
            return output_path
        except Exception as e:
            logger.error(f"Error saving code review results: {str(e)}")
            raise
    
    def get_critical_issues_summary(self, review_results: Dict[str, Any]) -> str:
        """
        Generate a summary of critical issues from the code review results.
        """
        if not review_results.get("critical_issues"):
            return "No critical issues found."
        
        summary = "Critical issues found:\n\n"
        for i, issue in enumerate(review_results["critical_issues"], 1):
            section_info = f" (Section: {issue.get('section_tag', 'N/A')})" if issue.get('section_tag') else ""
            summary += f"{i}. {issue.get('type', 'Unknown')} issue{section_info}: {issue.get('description', 'No description')}\n"
            summary += f"   Location: {issue.get('location', 'Unknown')}\n"
            summary += f"   Recommendation: {issue.get('recommendation', 'No recommendation')}\n\n"
        
        return summary

# Example usage
if __name__ == "__main__":
    # Example API key (would be provided securely in production)
    api_key = os.getenv("GOOGLE_API_KEY")
    
    if not api_key:
        logger.error("No Google API key found in environment variables")
        exit(1)
    
    # Initialize the code reviewer
    reviewer = CodeReviewer(api_key)
    
    # Perform code review
    review_results = reviewer.review_code(
        code_path="pipeline_run_outputs/generated_data_script.py",
        requirements_path="pipeline_run_outputs/generation_requirements.txt"
    )
    
    # Save the review results
    output_path = reviewer.save_review_results(review_results)
    
    # Print summary of critical issues
    critical_summary = reviewer.get_critical_issues_summary(review_results)
    print(f"Code review complete. Results saved to {output_path}")
    print("\nSummary:")
    print(f"Pass: {review_results.get('pass', False)}")
    print(f"Summary: {review_results.get('summary', 'No summary available')}")
    print("\nCritical Issues:")
    print(critical_summary)