import json
import os
import logging
from typing import Dict, List, Any, Optional, Tuple
import google.generativeai as genai #type: ignore

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('DataSynthesizer')

class CodeReviewer:
    """
    Component responsible for performing code review and static analysis
    of the generated data synthesis code using LLM (Gemini 2.5 Pro Preview).
    """
    
    def __init__(self, api_key: str):
        """
        Initialize the Code Reviewer component.
        
        Args:
            api_key: Google API key for accessing Gemini model
        """
        self.api_key = api_key
        
        # Configure the Gemini API
        genai.configure(api_key=api_key)
        
        # Initialize the model 
        self.model = genai.GenerativeModel(
            #model_name="models/gemini-2.5-pro-preview-05-06"
            model_name="models/gemini-2.5-flash-preview-04-17"
        )
        
        logger.info("Code Reviewer initialized with Gemini 2.5 Pro Preview model")
    
    def _load_generated_code(self, code_path: str) -> str:
        """
        Load the generated code file.
        
        Args:
            code_path: Path to the generated code file
            
        Returns:
            Generated code content as a string
        """
        logger.info(f"Loading generated code from {code_path}")
        try:
            with open(code_path, 'r', encoding='utf-8') as f:
                code = f.read()
            logger.info(f"Successfully loaded generated code ({len(code)} characters)")
            return code
        except Exception as e:
            logger.error(f"Error loading generated code: {str(e)}")
            raise
    
    def _load_requirements(self, requirements_path: str) -> str:
        """
        Load the requirements document generated by the LLM Analyzer.
        
        Args:
            requirements_path: Path to the requirements document
            
        Returns:
            Requirements document content as a string
        """
        logger.info(f"Loading requirements from {requirements_path}")
        try:
            with open(requirements_path, 'r', encoding='utf-8') as f:
                requirements = f.read()
            logger.info("Successfully loaded requirements document")
            return requirements
        except Exception as e:
            logger.error(f"Error loading requirements: {str(e)}")
            raise
    
    def _construct_code_review_prompt(self, code: str, requirements: str) -> str:
        """
        Construct a detailed prompt for the LLM to perform code review.
        """
        logger.info("Constructing code review prompt")
        
        # Construct the full prompt
        prompt = f"""
You are an expert Python code reviewer specializing in data generation and synthesis code. 
Your task is to perform a thorough static analysis of the provided Python code to identify
issues, errors, and ensure it meets the specified requirements.

# Code to Review
```python
{code}
```

# Requirements to Validate Against
```
{requirements}
```

# Review Instructions

Please perform a comprehensive code review focusing on the following aspects (the coder was only given the same Requirements as you do, so if the code improvise due to lack of information, don't be too hard):

1. **Syntax and Basic Structure**:
   - Check for syntax errors
   - Validate import statements
   - Examine function and class definitions
   - Look for indentation or formatting issues

2. **Functional Completeness**:
   - Does the code implement all columns specified in the requirements?
   - Does the code follow the requirements for each column?
   - Are relationships between columns properly handled?
   - Does the code adhere to the column generation order in the requirements?

3. **Implementation of Requirements**:
   - For each column in the requirements, verify that:
     - The specified generation strategy is used
     - Required libraries and providers are properly imported and used
     - Patterns and constraints are respected
     - Dependencies between columns are handled correctly

4. **Data Quality Checks**:
   - Are there mechanisms to validate generated data?
   - Are edge cases handled properly?
   - Is there adequate error handling?

5. **Execution Readiness**:
   - Are there potential runtime errors or exceptions?
   - Is the code runnable without modifications?
   - Are there any missing dependencies or assumptions?

6. **Efficiency and Best Practices**:
   - Are there any performance concerns?
   - Does the code follow Python best practices?
   - Is there adequate commenting and documentation?

# Output Format

Please provide your analysis in the following JSON format:

```json
{{
  "pass": true/false,
  "summary": "Short summary of the review",
  "critical_issues": [
    {{
      "type": "syntax/functional/requirements/etc.",
      "description": "Detailed description of the issue",
      "traceback": "If this issue directly relates to an error message or traceback seen during execution (check 'Code Execution Context' if provided), include the specific error message.",
      "location": "Line numbers or function name",
      "severity": "critical",
      "recommendation": "How to fix the issue (provide actionable solutions)"
    }}
  ],
  "non_critical_issues": [
    {{
      "type": "performance/style/etc.",
      "description": "Detailed description of the issue",
      "location": "Line numbers or function name",
      "severity": "warning/suggestion",
      "recommendation": "How to improve this aspect"
    }}
  ],
  "requirement_validations": [
    {{
      "requirement": "Column X with pattern Y",
      "implemented": true/false,
      "notes": "Notes on implementation correctness"
    }}
  ],
  "overall_assessment": "Detailed overall assessment of code quality and compliance",
  "passed_columns": ["COLUMN_A_NAME_IF_IT_PASSED", "COLUMN_B_NAME_IF_IT_PASSED"] // List of column names whose generation logic passed validation and should not be modified. Provide an empty list if none definitively passed.
}}
```

Ensure your JSON output is properly formatted and valid. The code "passes" static analysis if it has no critical issues that would prevent successful execution or generation of data meeting the requirements.
"""
        return prompt
    
    def review_code(self, code_path: str, requirements_path: str, execution_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
    Perform static analysis and code review of the generated code,
    with optional execution/validation context.
        """
        logger.info("Starting code review process")

        code = self._load_generated_code(code_path)
        requirements = self._load_requirements(requirements_path)

        # Add execution context to the prompt
        exec_context_str = ""
        if execution_context:
            exec_context_str += "This is the result of the code execution, review the code with this context in mind and avoid giving contradicting feedback.\n"
            exec_context_str += "\n\n# Code Execution Context\n"
            exec_context_str += f"Execution success: {execution_context.get('success', False)}\n"
            exec_context_str += f"Execution summary: {execution_context.get('summary', '')}\n"

            if execution_context.get("traceback"):
                exec_context_str += f"\nTraceback:\n{execution_context['traceback']}\n"

            if execution_context.get("validations"):
                exec_context_str += "\nValidation Failures:\n"
                for v in execution_context["validations"]:
                    if not v.get("passed", True):
                        exec_context_str += f"- {v.get('name', '')} on {v.get('column', '')}: {v.get('details', '')}\n"

        # Add review guidance to encourage precise and non-redundant critique
        review_guidance = """
# Review Guidelines:
- Focus only on issues that would lead to execution failure or data validation failure.
- Avoid nitpicking or recommending stylistic changes unless necessary.
- If parts of the code are working and validated, acknowledge them.
- Populate the "passed_columns" field in the JSON output with a list of column names that passed validation. These are columns whose core data generation logic (creation-wise, not order-wise) met requirements and did not cause errors. If no columns definitively passed, provide an empty list [] in the "passed_columns" field.
"""

        prompt = exec_context_str + self._construct_code_review_prompt(code, requirements) + review_guidance

        try:
            logger.info("Sending review prompt to Gemini with execution context")
            response = self.model.generate_content(prompt)
            review_results = self._parse_code_review_response(response.text)
            logger.info("Successfully completed contextual code review")
            return review_results

        except Exception as e:
            logger.error(f"Error during review: {str(e)}")
            return {
                "pass": False,
                "summary": f"Error in review: {str(e)}",
                "critical_issues": [{
                    "type": "review_error",
                    "description": f"Review exception: {str(e)}",
                    "severity": "critical",
                    "recommendation": "Check model availability and inputs."
                }],
                "non_critical_issues": [],
                "overall_assessment": "Review failed.",
                "passed_columns": []
            }

    
    def _parse_code_review_response(self, response_text: str) -> Dict[str, Any]:
        """
        Parse the LLM response to extract structured code review results.
        """
        try:
            # Try to find and extract JSON content from the response
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response_text[json_start:json_end]
                review_results = json.loads(json_str)
                return review_results
            else:
                # If no JSON found, log an error and return error structure
                logger.error("No valid JSON found in LLM response")
                return {
                    "pass": False,
                    "summary": "Failed to parse LLM response",
                    "critical_issues": [{
                        "type": "parsing_error",
                        "description": "Could not parse LLM response as JSON",
                        "severity": "critical",
                        "recommendation": "Check LLM prompt and response format"
                    }],
                    "overall_assessment": "Code review failed due to response parsing error"
                }
                
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing LLM response as JSON: {str(e)}")
            logger.debug(f"Response text: {response_text}")
            # Return error structure on JSON parse error
            return {
                "pass": False,
                "summary": "Failed to parse LLM response as JSON",
                "critical_issues": [{
                    "type": "json_decode_error",
                    "description": f"JSON decode error: {str(e)}",
                    "severity": "critical",
                    "recommendation": "Check LLM response format"
                }],
                "overall_assessment": "Code review failed due to JSON parsing error"
            }
    
    def save_review_results(self, review_results: Dict[str, Any], output_path: str = "pipeline_run_outputs/code_review.json") -> str:
        """
        Save the code review results to a file.
            
        Returns:
            Path to the saved file
        """
        logger.info(f"Saving code review results to {output_path}")
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(review_results, f, ensure_ascii=False, indent=2)
            logger.info(f"Code review results successfully saved to {output_path}")
            return output_path
        except Exception as e:
            logger.error(f"Error saving code review results: {str(e)}")
            raise
    
    def get_critical_issues_summary(self, review_results: Dict[str, Any]) -> str:
        """
        Generate a summary of critical issues from the code review results.
        
        Args:
            review_results: Dictionary containing code review results
            
        Returns:
            String summary of critical issues
        """
        if not review_results.get("critical_issues"):
            return "No critical issues found."
        
        summary = "Critical issues found:\n\n"
        for i, issue in enumerate(review_results["critical_issues"], 1):
            summary += f"{i}. {issue.get('type', 'Unknown')} issue: {issue.get('description', 'No description')}\n"
            summary += f"   Location: {issue.get('location', 'Unknown')}\n"
            summary += f"   Recommendation: {issue.get('recommendation', 'No recommendation')}\n\n"
        
        return summary

# Example usage
if __name__ == "__main__":
    # Example API key (would be provided securely in production)
    api_key = os.getenv("GOOGLE_API_KEY")
    
    if not api_key:
        logger.error("No Google API key found in environment variables")
        exit(1)
    
    # Initialize the code reviewer
    reviewer = CodeReviewer(api_key)
    
    # Perform code review
    review_results = reviewer.review_code(
        code_path="pipeline_run_outputs/generated_data_script.py",
        requirements_path="pipeline_run_outputs/generation_requirements.txt"
    )
    
    # Save the review results
    output_path = reviewer.save_review_results(review_results)
    
    # Print summary of critical issues
    critical_summary = reviewer.get_critical_issues_summary(review_results)
    print(f"Code review complete. Results saved to {output_path}")
    print("\nSummary:")
    print(f"Pass: {review_results.get('pass', False)}")
    print(f"Summary: {review_results.get('summary', 'No summary available')}")
    print("\nCritical Issues:")
    print(critical_summary)